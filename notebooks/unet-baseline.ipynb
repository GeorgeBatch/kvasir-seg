{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Put all libraries and packages at top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for updating the custom imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files and system\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with images\n",
    "import cv2\n",
    "import imageio as iio\n",
    "import scipy.ndimage\n",
    "import skimage.transform\n",
    "\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchsummary\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..')\n",
    "\n",
    "# losses\n",
    "from utils.metrics import iou_pytorch_eval, IoULoss, IoUBCELoss\n",
    "from utils.metrics import iou_pytorch_test, dice_pytorch_test, precision_pytorch_test, recall_pytorch_test, fbeta_pytorch_test, accuracy_pytorch_test\n",
    "\n",
    "# dataset\n",
    "from utils.dataset import myDataSet\n",
    "\n",
    "# transforms\n",
    "from utils.transforms import SIZE, resize_transform, train_transforms, test_transforms\n",
    "\n",
    "sys.path.insert(0, './notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../models')\n",
    "\n",
    "# models\n",
    "from unet import UNet\n",
    "\n",
    "sys.path.insert(0, '../notebooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fix a seed for reproducibility, and choose the DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed= 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # select device for training, i.e. gpu or cpu\n",
    "print(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set your transforms, e.g, normalisation, resizing, rotation, flip, padding etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SIZE)\n",
    "print(resize_transform)\n",
    "print(train_transforms)\n",
    "print(test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Make your train and validation data loader with option to use different preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_images = \"../data/train-val/images\"\n",
    "path_masks = \"../data/train-val/masks\"\n",
    "\n",
    "\n",
    "# pre-defined split\n",
    "with open('../train-val-split/train.txt', 'r') as f:\n",
    "    ids_train = [l.strip()+'.jpg' for l in f]\n",
    "with open('../train-val-split/val.txt', 'r') as f:\n",
    "    ids_val = [l.strip()+'.jpg' for l in f]\n",
    "\n",
    "\n",
    "custom_dataset_train = myDataSet(ids_train, path_images, path_masks, transforms=test_transforms)\n",
    "custom_dataset_val = myDataSet(ids_val, path_images, path_masks, transforms=test_transforms)\n",
    "\n",
    "print(\"My custom training-dataset has {} elements\".format(len(custom_dataset_train)))\n",
    "print(\"My custom validation-dataset has {} elements\".format(len(custom_dataset_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "\n",
    "# Create dataloaders from datasets with the native pytorch functions\n",
    "dataloader_train = torch.utils.data.DataLoader(custom_dataset_train, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "dataloader_val = torch.utils.data.DataLoader(custom_dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example images.\n",
    "image_number = 23\n",
    "img, mask = custom_dataset_train.__getitem__(image_number)\n",
    "\n",
    "\n",
    "# image\n",
    "plt.figure()\n",
    "plt.imshow(img.mean(0)) # 3 channels, take mean\n",
    "\n",
    "# mask\n",
    "plt.figure()\n",
    "plt.imshow(mask[0, :, :]) # 1 channel, take it\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset_train.__getitem__(image_number)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset_train.__getitem__(image_number)[1].shape, np.unique(custom_dataset_train.__getitem__(image_number)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin training\n",
    "model = UNet(channel_in=3, channel_out=1)\n",
    "model = model.to(DEVICE) # load model to DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for the training\n",
    "epochs = 100\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimiser and criterion for the training. You can try different ones to see which works best for your data and task\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = 1e-4, weight_decay = 1e-8)\n",
    "\n",
    "# criterion = IoULoss()\n",
    "# model_name = 'UNet_IoULoss_baseline'\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# model_name = 'UNet_BCELoss_baseline'\n",
    "\n",
    "criterion = IoUBCELoss()\n",
    "model_name = 'UNet_IoUBCELoss_baseline'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_iou = 0\n",
    "best_loss = np.Inf\n",
    "best_epoch = -1\n",
    "state = {}\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    running_iou = 0\n",
    "    # Train\n",
    "    model.train()\n",
    "    for i, (imgs, masks) in enumerate(dataloader_train):\n",
    "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "        \n",
    "        prediction = model(imgs)\n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        loss = criterion(prediction, masks)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        print(\"\\r Epoch: {} of {}, Iter.: {} of {}, Loss: {:.6f}\".format(epoch, epochs, i, len(dataloader_train), running_loss/(i+1)), end=\"\")\n",
    "        \n",
    "        running_iou += iou_pytorch_eval(prediction, masks)\n",
    "        print(\"\\r Epoch: {} of {}, Iter.: {} of {}, IoU:  {:.6f}\".format(epoch, epochs, i, len(dataloader_train), running_iou/(i+1)), end=\"\")\n",
    "        \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_iou = 0\n",
    "    for i, (imgs, masks) in enumerate(dataloader_val):\n",
    "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "        \n",
    "        prediction = model(imgs)\n",
    "        loss = criterion(prediction, masks)\n",
    "        val_loss += loss.item()\n",
    "        print(\"\\r Epoch: {} of {}, Iter.: {} of {}, Loss: {:.6f}, Val. Loss: {:.6f}\".format(epoch, epochs, len(dataloader_train), len(dataloader_train), running_loss/len(dataloader_train), val_loss/(i+1)), end=\"\")\n",
    "        \n",
    "        val_iou += iou_pytorch_eval(prediction, masks)\n",
    "        print(\"\\r Epoch: {} of {}, Iter.: {} of {}, IoU: {:.6f}, Val. IoU: {:.6f}\".format(epoch, epochs, len(dataloader_train), len(dataloader_train), running_iou/len(dataloader_train), val_iou/(i+1)), end=\"\")\n",
    "    \n",
    "    \n",
    "    # compute overall epoch losses\n",
    "    epoch_train_loss = running_loss/len(dataloader_train)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    epoch_val_loss = val_loss/len(dataloader_val)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    # compute overall epoch iou-s\n",
    "    epoch_train_iou = running_iou/len(dataloader_train)\n",
    "    epoch_val_iou = val_iou/len(dataloader_val)\n",
    "    \n",
    "    print(\"\\r Epoch: {} of {}, Iter.: {} of {}, Train Loss: {:.6f}, IoU: {:.6f}\".format(epoch, epochs, len(dataloader_train), len(dataloader_train), epoch_train_loss, epoch_train_iou))\n",
    "    print(\"\\r Epoch: {} of {}, Iter.: {} of {}, Valid Loss: {:.6f}, IoU: {:.6f}\".format(epoch, epochs, len(dataloader_train), len(dataloader_train), epoch_val_loss, epoch_val_iou))\n",
    "    \n",
    "    \n",
    "    # plot\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    plt.plot(np.arange(len(train_losses)), train_losses, label=f'Train, loss: {epoch_train_loss:.4f}, IoU: {epoch_train_iou:.4f}', linewidth=3)\n",
    "    plt.plot(np.arange(len(val_losses)), val_losses, label=f'Valid, loss: {epoch_val_loss:.4f}, IoU: {epoch_val_iou:.4f}', linewidth=3)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Epoch {epoch}')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    # save if best results or break is has not improved for {patience} number of epochs\n",
    "    best_iou = max(best_iou, epoch_val_iou)\n",
    "    best_loss = min(best_loss, epoch_val_loss)\n",
    "    best_epoch = epoch if best_iou == epoch_val_iou else best_epoch\n",
    "    \n",
    "    # record losses\n",
    "    state['train_losses'] = train_losses\n",
    "    state['val_losses'] = val_losses\n",
    "    \n",
    "    if best_epoch == epoch:\n",
    "        # print('Saving..')\n",
    "        state['net'] = model.state_dict()\n",
    "        state['iou'] = best_iou\n",
    "        state['epoch'] = epoch\n",
    "            \n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, f'../checkpoints/ckpt_{model_name}.pth')\n",
    "    \n",
    "    elif best_epoch + patience < epoch:\n",
    "        print(f\"\\nEarly stopping. Target criteria has not improved for {patience} epochs.\\n\")\n",
    "        break\n",
    "        \n",
    "        \n",
    "# load once more and write all the losses down (othw can miss the last 10)\n",
    "state = torch.load(f'../checkpoints/ckpt_{model_name}.pth')\n",
    "state['train_losses'] = train_losses\n",
    "state['val_losses'] = val_losses\n",
    "torch.save(state, f'../checkpoints/ckpt_{model_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch, best_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate validation performance (256*256 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ../checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin training\n",
    "model = UNet(channel_in=3, channel_out=1)\n",
    "model = model.to(DEVICE) # load model to DEVICE\n",
    "\n",
    "# load best weights and put into the evaluation mode\n",
    "print(model_name)\n",
    "model.load_state_dict(torch.load(f'../checkpoints/ckpt_{model_name}.pth')['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best epoch:', torch.load(f'../checkpoints/ckpt_{model_name}.pth')['epoch'])\n",
    "print(f'Validation IoU ({_size[0]}x{_size[0]}):', torch.load(f'../checkpoints/ckpt_{model_name}.pth')['iou'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, optim, criterion, dataloader, epoch, device):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, criterion, dataloader, epoch, device, best_acc, model_name='model'):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Begin training\n",
    "# model = UNet(channel_in=3, channel_out=1)\n",
    "# model = model.to(DEVICE)\n",
    "\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "# best_iou = 0\n",
    "# best_epoch = -1\n",
    "\n",
    "# for epoch in notebook.tqdm(range(30)):\n",
    "#     train_loss, train_iou = train(model, optimiser, criterion, dataloader_train, epoch, DEVICE)\n",
    "#     train_losses.append(train_loss)\n",
    "    \n",
    "#     test_loss, test_iou = test(model, criterion, dataloader_val, epoch, DEVICE, best_iou, model_name='Unet_BCEWithLogitsLoss')\n",
    "#     #scheduler.step(test_loss) # not always needed\n",
    "#     test_losses.append(test_loss)\n",
    "    \n",
    "#     best_iou = max(best_iou, test_iou)\n",
    "#     best_epoch = epoch if best_iou == test_iou else best_epoch\n",
    "\n",
    "#     clear_output()\n",
    "    \n",
    "#     plt.figure(figsize=(18, 9))\n",
    "#     plt.plot(np.arange(len(train_losses)), train_losses, label=f'Train, loss: {train_loss:.4f}, IoU: {train_iou}%')\n",
    "#     plt.plot(np.arange(len(test_losses)), test_losses, label=f'Test, loss: {test_loss:.4f}, IoU: {test_iou}%')\n",
    "#     plt.title(f'Epoch {epoch}')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
